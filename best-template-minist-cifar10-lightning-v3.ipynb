{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch_lightning pandas seaborn torch torchmetrics torchvision \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-19T21:14:13.824469Z","iopub.execute_input":"2022-07-19T21:14:13.824940Z","iopub.status.idle":"2022-07-19T21:14:26.135013Z","shell.execute_reply.started":"2022-07-19T21:14:13.824838Z","shell.execute_reply":"2022-07-19T21:14:26.133910Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport pytorch_lightning\nfrom IPython.core.display import display\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger \nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\n# set the current directory as working directory\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n# setting of batch size\nBATCH_SIZE = 256 if torch.cuda.is_available() else 64\n\nclass LitMNIST(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS):\n        super().__init__()\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        #pytorch_lightning.utilities.seed.seed_everything(1234,workers=True)\n\n\n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (1, 28, 28)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.1307,), (0.3081,))\n                #transforms.Normalize((0.5,), (0.5,))\n            ]\n        )\n        #self.model=model\n        # Define PyTorch model\n        self.model = nn.Sequential(\n            #CNN-v2\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            #nn.MaxPool2d(2, 2),  # output: 64 x 14 x 14\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # output: 64 x 14 x 14\n\n            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            # nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            # nn.ReLU(),\n            # #nn.MaxPool2d(2, 2),  # output: 128 x 7 x 7\n\n            # nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            # nn.ReLU(),\n            # nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            # nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # output: 256 x 7 x 7\n\n            nn.Flatten(),\n            nn.Linear(256 * 7 * 7, 512),\n            nn.ReLU(),\n            #nn.Linear(1024, 512),\n            #nn.ReLU(),\n            #nn.Dropout(inplace=True),\n            nn.Linear(512, self.num_classes),\n\n        )\n        self.train_accuracy = Accuracy()\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.train_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n        return [optimizer], [lr_scheduler]\n\n    def validation_epoch_end(self, outputs):\n\n        return \n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=BATCH_SIZE)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=BATCH_SIZE)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=BATCH_SIZE)\n\n\nstart=datetime.now()\nmodel = LitMNIST()\n\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n#logger = TensorBoardLogger(\"lightning_logs\", name=None)\ntrainer = Trainer(\n    fast_dev_run=False,\n    #deterministic=True,\n    gradient_clip_val=1,\n    auto_lr_find=True,\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=3,\n    callbacks=[TQDMProgressBar(refresh_rate=20)],\n    #logger=logger,\n    logger=CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\nprint()\nprint(f\"The time cost in training model: {datetime.now()-start}\")\nprint()\ntrainer.test()\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\ndisplay(metrics.dropna(axis=1, how=\"all\"))\n#sn.relplot(data=metrics, kind=\"line\")","metadata":{"execution":{"iopub.status.busy":"2022-07-19T21:14:46.940301Z","iopub.execute_input":"2022-07-19T21:14:46.940646Z","iopub.status.idle":"2022-07-19T21:15:54.360598Z","shell.execute_reply.started":"2022-07-19T21:14:46.940616Z","shell.execute_reply":"2022-07-19T21:15:54.359353Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport pytorch_lightning\nfrom IPython.core.display import display\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger \nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.datasets import ImageFolder\n\n\nfrom torchvision.datasets.utils import download_url\nimport tarfile\n\n# Dowload the dataset\ndataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\ndownload_url(dataset_url, '.')\n\n# Extract from archive\nwith tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n    tar.extractall(path='./data')\n    \n# Look into the data directory\ndata_dir = './data/cifar10'\nprint(os.listdir(data_dir))\nclasses = os.listdir(data_dir + \"/train\")\nprint(classes)\n\n\n# set the current directory as working directory\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n# setting of batch size\nBATCH_SIZE = 200 if torch.cuda.is_available() else 64\n\n# #define my more complex NN model\n# class ResNet(nn.Module):\n#   def __init__(self):\n#     super().__init__()\n#     self.l1=nn.Linear(28*28,64)\n#     self.l2=nn.Linear(64,64)\n#     self.l3=nn.Linear(64,10)\n#     self.do=nn.Dropout(0.1)\n#\n#   def forward(self,x):\n#     h1=nn.functional.relu(self.l1(x))\n#     h2=nn.functional.relu(self.l2(h1))\n#     do=self.do(h2+h1) # complete the residue step: output + input at this layer l2\n#     logits =self.l3(do)\n#     return logits\n#\n# mymodel=ResNet()\n\n\n\nclass LitCIFAR10(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS):\n        super().__init__()\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        \n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (3, 32, 32)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [   #transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n                #transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            ]\n        )\n        \n        # Define PyTorch model\n        self.model = nn.Sequential(\n#Fully connected NN\n            # nn.Flatten(),\n            # nn.Linear(channels * width * height,64),\n            # nn.ReLU(),\n            # nn.Dropout(0.1),\n            # nn.Linear(64, 64),\n            # nn.ReLU(),\n            # nn.Dropout(0.1),\n            # nn.Linear(64, self.num_classes),\n#CNN-v1\n            # nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            # nn.ReLU(),\n            # #nn.MaxPool2d(2, 2),  # output: 64 x 14 x 14\n            # nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            # nn.ReLU(),\n            # nn.MaxPool2d(2, 2),  # output: 64 x 14 x 14\n\n            # nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            # nn.ReLU(),\n            # nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            # nn.ReLU(),\n            # #nn.MaxPool2d(2, 2),  # output: 128 x 7 x 7\n\n            # nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            # nn.ReLU(),\n            # nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            # nn.ReLU(),\n            # nn.MaxPool2d(2, 2),  # output: 128 x 7 x 7\n\n            # nn.Flatten(),\n            # nn.Linear(256 * 7 * 7, 1024),\n            # nn.ReLU(),\n            # nn.Linear(1024, 512),\n            # nn.ReLU(),\n            # #nn.Dropout(inplace=True),\n            # nn.Linear(512, self.num_classes),\n\n#CNN-v2\n            # nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            # nn.ReLU(),\n            # #nn.MaxPool2d(2, 2),  # output: 64 x 14 x 14\n            # nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            # nn.ReLU(),\n            # nn.MaxPool2d(2, 2),  # output: 64 x 16 x 16\n\n            # nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n            # nn.ReLU(),\n            # # nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            # # nn.ReLU(),\n            # # #nn.MaxPool2d(2, 2),  # output: 128 x 7 x 7\n\n            # # nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            # # nn.ReLU(),\n            # # nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            # # nn.ReLU(),\n            # nn.MaxPool2d(2, 2),  # output: 256 x 8 x 8\n\n            # nn.Flatten(),\n            # nn.Linear(256 * 8 * 8, 512),\n            # nn.ReLU(),\n            # #nn.Linear(1024, 512),\n            # #nn.ReLU(),\n            # #nn.Dropout(0.2),\n            # nn.Linear(512, self.num_classes),\n\n#CNN-v3\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 512 x 2 x 2\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 1024 x 1 x 1\n\n            nn.Flatten(), \n#             nn.Linear(256*2*2, 1024),\n#             nn.ReLU(),\n#             nn.Linear(1024, 512),\n#             nn.ReLU(),\n            #nn.Dropout(inplace=True),\n            nn.Dropout(p=0.2, inplace=False),\n            nn.Linear(1024, 10),\n\n\n\n#RNN does not work well here\n\n          # #nn.LSTM(128,256),\n          # #nn.Dropout(0.2),\n          # nn.LSTM(256,128),\n          # #nn.Dropout(0.2),\n          # nn.Linear(128,32),\n          # nn.Linear(32,self.num_classes),\n\n        )\n        self.train_accuracy = Accuracy()\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        #loss = F.nll_loss(logits, y)\n        loss = F.cross_entropy(logits, y) # Calculate loss\n        preds = torch.argmax(logits, dim=1)\n        self.train_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        #loss = F.nll_loss(logits, y)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        #loss = F.nll_loss(logits, y)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n         # Set up cutom optimizer with weight decay\n        #optimizer = opt_func(model.parameters(), 0.01, weight_decay=weight_decay)\n        # Set up one-cycle learning rate scheduler\n        #lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.01, steps_per_epoch=None)\n        optimizer = torch.optim.Adam(self.model.parameters(),weight_decay=1e-4,lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=15)\n#         optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n#         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n        return [optimizer], [lr_scheduler]\n\n    def validation_epoch_end(self, outputs):\n\n        return \n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        CIFAR10(self.data_dir, train=True, download=True)\n        CIFAR10(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n\n        # Assign train/val datasets for use in dataloaders\n#         if stage == \"fit\" or stage is None:\n#             datasets = CIFAR10(self.data_dir, train=True, transform=self.transform)\n#             self.train_data, self.val_data = random_split(datasets, [45000, 5000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.test_data = CIFAR10(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        # PyTorch datasets\n        data_dir = './data/cifar10'\n        stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        train_tfms =transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n                         transforms.RandomHorizontalFlip(), \n                         # tt.RandomRotate\n                         # tt.RandomResizedCrop(256, scale=(0.5,0.9), ratio=(1, 1)), \n                         # tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                         transforms.ToTensor(), \n                         transforms.Normalize(*stats,inplace=True)])\n\n        train_ds = ImageFolder(data_dir+'/train', train_tfms)\n        train_dl = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n#         print(\"training size:\")\n#         print(len(train_dl)\n        return train_dl\n\n    def val_dataloader(self):\n        data_dir = './data/cifar10'\n        stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        valid_tfms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(*stats)])\n        valid_ds = ImageFolder(data_dir+'/test', valid_tfms)\n        valid_dl = DataLoader(valid_ds, BATCH_SIZE, num_workers=2, pin_memory=True)\n#         print(\"validation size:\")\n#         print(len(valid_dl))\n        return valid_dl\n\n    def test_dataloader(self):\n        return DataLoader(self.test_data, batch_size=BATCH_SIZE)\n\n\nstart=datetime.now()\nmodel = LitCIFAR10()\n\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n#logger = TensorBoardLogger(\"lightning_logs\", name=None)\n\ntrainer = Trainer(\n    precision=16,\n    #fast_dev_run=False,\n    #deterministic=True,\n    gradient_clip_val=0.1,\n    #auto_lr_find=True,\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=10,\n    callbacks=[TQDMProgressBar(refresh_rate=20)],\n    #logger=logger,\n    logger=CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\nprint()\nprint(f\"The time cost in training model: {datetime.now()-start}\")\nprint()\ntrainer.test()\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\ndisplay(metrics.dropna(axis=1, how=\"all\"))\n#sn.relplot(data=metrics, kind=\"line\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-19T21:17:01.470661Z","iopub.execute_input":"2022-07-19T21:17:01.471087Z","iopub.status.idle":"2022-07-19T21:24:04.621264Z","shell.execute_reply.started":"2022-07-19T21:17:01.471055Z","shell.execute_reply":"2022-07-19T21:24:04.619784Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport pytorch_lightning\nfrom IPython.core.display import display\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger \nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.datasets import ImageFolder\n\n\n#from torchvision.datasets.utils import download_url\n#import tarfile\n\n# set the current directory as working directory\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n# setting of batch size\nBATCH_SIZE = 200 if torch.cuda.is_available() else 64\n\nclass LitCIFAR10(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS):\n        super().__init__()\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        \n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (3, 32, 32)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [   transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            ]\n        )\n        \n        # Define PyTorch model\n        self.model = nn.Sequential(\n            #CNN-v3\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 512 x 2 x 2\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 1024 x 1 x 1\n\n            nn.Flatten(), \n            nn.Dropout(p=0.2, inplace=False),\n            nn.Linear(1024, 10),\n        )\n        self.train_accuracy = Accuracy()\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y) # Calculate loss\n        preds = torch.argmax(logits, dim=1)\n        self.train_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n         # Set up cutom optimizer with weight decay\n        optimizer = torch.optim.Adam(self.model.parameters(),weight_decay=1e-4,lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=15)\n        return [optimizer], [lr_scheduler]\n\n    def validation_epoch_end(self, outputs):\n\n        return \n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        CIFAR10(self.data_dir, train=True, download=True)\n        CIFAR10(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            train_tfms =transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n                         transforms.RandomHorizontalFlip(), \n                         transforms.ToTensor(), \n                         transforms.Normalize(*stats,inplace=True)])\n            datasets = CIFAR10(self.data_dir, train=True, transform=train_tfms) # image augumention \n            self.train_data, self.val_data = random_split(datasets, [45000, 5000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.test_data = CIFAR10(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        train_dl = DataLoader(self.train_data, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n        return train_dl\n\n    def val_dataloader(self):\n        valid_dl = DataLoader(self.val_data, BATCH_SIZE, num_workers=2, pin_memory=True)\n        return valid_dl\n\n    def test_dataloader(self):\n        return DataLoader(self.test_data, batch_size=BATCH_SIZE)\n\n\nstart=datetime.now()\nmodel = LitCIFAR10()\n\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n#logger = TensorBoardLogger(\"lightning_logs\", name=None)\n\ntrainer = Trainer(\n    precision=16,\n    #fast_dev_run=False,\n    #deterministic=True,\n    gradient_clip_val=0.1,\n    #auto_lr_find=True,\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=50,\n    callbacks=[TQDMProgressBar(refresh_rate=20)],\n    #logger=logger,\n    logger=CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\nprint()\nprint(f\"The time cost in training model: {datetime.now()-start}\")\nprint()\ntrainer.test()\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\ndisplay(metrics.dropna(axis=1, how=\"all\"))\n#sn.relplot(data=metrics, kind=\"line\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-19T21:47:58.159311Z","iopub.execute_input":"2022-07-19T21:47:58.159714Z","iopub.status.idle":"2022-07-19T22:12:50.489948Z","shell.execute_reply.started":"2022-07-19T21:47:58.159677Z","shell.execute_reply":"2022-07-19T22:12:50.488851Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport pytorch_lightning\nfrom IPython.core.display import display\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger \nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.datasets import ImageFolder\n\n\n#from torchvision.datasets.utils import download_url\n#import tarfile\n\n# set the current directory as working directory\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n# setting of batch size\nBATCH_SIZE = 200 if torch.cuda.is_available() else 64\n\nclass LitCIFAR10(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS):\n        super().__init__()\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        \n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (3, 32, 32)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [   transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            ]\n        )\n        \n        # Define PyTorch model\n        self.model = nn.Sequential(\n            #CNN-v3\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 512 x 2 x 2\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 1024 x 1 x 1\n\n            nn.Flatten(), \n            nn.Dropout(p=0.2, inplace=False),\n            nn.Linear(1024, 10), #fully connected network\n        )\n        self.train_accuracy = Accuracy()\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y) # Calculate loss\n        preds = torch.argmax(logits, dim=1)\n        self.train_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n         # Set up cutom optimizer with weight decay\n        optimizer = torch.optim.Adam(self.model.parameters(),weight_decay=1e-4,lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=15)\n        return [optimizer], [lr_scheduler]\n\n    def validation_epoch_end(self, outputs):\n\n        return \n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        CIFAR10(self.data_dir, train=True, download=True)\n        CIFAR10(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        datasets = CIFAR10(self.data_dir, train=True, transform=self.transform) # image augumention \n        self.train_data, self.val_data = random_split(datasets, [49000, 1000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.test_data = CIFAR10(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        train_dl = DataLoader(self.train_data, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n        return train_dl\n\n    def val_dataloader(self):\n        valid_dl = DataLoader(self.val_data, BATCH_SIZE, num_workers=2, pin_memory=True)\n        return valid_dl\n\n    def test_dataloader(self):\n        return DataLoader(self.test_data, batch_size=BATCH_SIZE)\n\n\nstart=datetime.now()\nmodel = LitCIFAR10()\n\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n#logger = TensorBoardLogger(\"lightning_logs\", name=None)\n\ntrainer = Trainer(\n    precision=16,\n    #fast_dev_run=False,\n    #deterministic=True,\n    gradient_clip_val=0.1,\n    #auto_lr_find=True,\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=50,\n    callbacks=[TQDMProgressBar(refresh_rate=20)],\n    #logger=logger,\n    logger=CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\nprint()\nprint(f\"The time cost in training model: {datetime.now()-start}\")\nprint()\ntrainer.test()\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\ndisplay(metrics.dropna(axis=1, how=\"all\"))\n#sn.relplot(data=metrics, kind=\"line\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-19T22:40:55.886650Z","iopub.execute_input":"2022-07-19T22:40:55.887052Z","iopub.status.idle":"2022-07-19T22:54:39.321903Z","shell.execute_reply.started":"2022-07-19T22:40:55.887017Z","shell.execute_reply":"2022-07-19T22:54:39.320768Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport pytorch_lightning\nfrom IPython.core.display import display\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger \nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.datasets import ImageFolder\n\n\n#from torchvision.datasets.utils import download_url\n#import tarfile\n\n# set the current directory as working directory\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n# setting of batch size\nBATCH_SIZE = 200 if torch.cuda.is_available() else 64\n\nclass LitCIFAR10(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS):\n        super().__init__()\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        \n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (3, 32, 32)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [   transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            ]\n        )\n        \n        # Define PyTorch model\n        self.model = nn.Sequential(\n            #CNN-v3\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 512 x 2 x 2\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 1024 x 1 x 1\n\n            nn.Flatten(), \n            nn.Dropout(p=0.2, inplace=False),\n            nn.Linear(1024, 10),\n        )\n        self.train_accuracy = Accuracy()\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y) # Calculate loss\n        preds = torch.argmax(logits, dim=1)\n        self.train_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n         # Set up cutom optimizer with weight decay\n        optimizer = torch.optim.Adam(self.model.parameters(),weight_decay=1e-4,lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=15)\n        return [optimizer], [lr_scheduler]\n\n    def validation_epoch_end(self, outputs):\n\n        return \n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        CIFAR10(self.data_dir, train=True, download=True)\n        CIFAR10(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            train_tfms =transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n                         transforms.RandomHorizontalFlip(), \n                         transforms.ToTensor(), \n                         transforms.Normalize(*stats,inplace=True)])\n            datasets = CIFAR10(self.data_dir, train=True, transform=train_tfms) # image augumention \n            self.train_data, self.val_data = random_split(datasets, [49000, 1000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.test_data = CIFAR10(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        train_dl = DataLoader(self.train_data, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n        return train_dl\n\n    def val_dataloader(self):\n        valid_dl = DataLoader(self.val_data, BATCH_SIZE, num_workers=2, pin_memory=True)\n        return valid_dl\n\n    def test_dataloader(self):\n        return DataLoader(self.test_data, batch_size=BATCH_SIZE)\n\n\nstart=datetime.now()\nmodel = LitCIFAR10()\n\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n#logger = TensorBoardLogger(\"lightning_logs\", name=None)\n\ntrainer = Trainer(\n    precision=16,\n    #fast_dev_run=False,\n    #deterministic=True,\n    gradient_clip_val=0.1,\n    #auto_lr_find=True,\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=50,\n    callbacks=[TQDMProgressBar(refresh_rate=20)],\n    #logger=logger,\n    logger=CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\nprint()\nprint(f\"The time cost in training model: {datetime.now()-start}\")\nprint()\ntrainer.test()\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\ndisplay(metrics.dropna(axis=1, how=\"all\"))\n#sn.relplot(data=metrics, kind=\"line\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-19T23:00:37.450155Z","iopub.execute_input":"2022-07-19T23:00:37.450538Z","iopub.status.idle":"2022-07-19T23:24:33.912375Z","shell.execute_reply.started":"2022-07-19T23:00:37.450506Z","shell.execute_reply":"2022-07-19T23:24:33.911237Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport pytorch_lightning\nfrom IPython.core.display import display\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger \nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.datasets import ImageFolder\n\n\n#from torchvision.datasets.utils import download_url\n#import tarfile\n\n# set the current directory as working directory\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n# setting of batch size\nBATCH_SIZE = 200 if torch.cuda.is_available() else 64\n\nclass LitCIFAR10(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS):\n        super().__init__()\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        \n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (3, 32, 32)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [   transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            ]\n        )\n        \n        # Define PyTorch model\n        self.model = nn.Sequential(\n            #CNN-v3\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:512 x 2 x 2\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:1024 x 1 x 1\n\n            nn.Flatten(), \n            nn.Dropout(p=0.2, inplace=False),\n            nn.Linear(1024, 10),\n        )\n        self.train_accuracy = Accuracy()\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y) # Calculate loss\n        preds = torch.argmax(logits, dim=1)\n        self.train_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n         # Set up cutom optimizer with weight decay\n        optimizer = torch.optim.Adam(self.model.parameters(),weight_decay=1e-4,lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=15)\n        return [optimizer], [lr_scheduler]\n\n    def validation_epoch_end(self, outputs):\n\n        return \n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        CIFAR10(self.data_dir, train=True, download=True)\n        CIFAR10(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            train_tfms =transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n                         transforms.RandomHorizontalFlip(), \n                         transforms.ToTensor(), \n                         transforms.Normalize(*stats,inplace=True)])\n            datasets = CIFAR10(self.data_dir, train=True, transform=train_tfms) # image augumention \n            self.train_data, self.val_data = random_split(datasets, [49000, 1000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.test_data = CIFAR10(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        train_dl = DataLoader(self.train_data, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n        return train_dl\n\n    def val_dataloader(self):\n        valid_dl = DataLoader(self.val_data, BATCH_SIZE, num_workers=2, pin_memory=True)\n        return valid_dl\n\n    def test_dataloader(self):\n        return DataLoader(self.test_data, batch_size=BATCH_SIZE)\n\n\nstart=datetime.now()\nmodel = LitCIFAR10()\n\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n#logger = TensorBoardLogger(\"lightning_logs\", name=None)\n\ntrainer = Trainer(\n    precision=16,\n    #fast_dev_run=False,\n    #deterministic=True,\n    gradient_clip_val=0.2,\n    #auto_lr_find=True,\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=50,\n    callbacks=[TQDMProgressBar(refresh_rate=20)],\n    #logger=logger,\n    logger=CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\nprint()\nprint(f\"The time cost in training model: {datetime.now()-start}\")\nprint()\ntrainer.test()\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\ndisplay(metrics.dropna(axis=1, how=\"all\"))\n#sn.relplot(data=metrics, kind=\"line\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-19T23:30:39.854704Z","iopub.execute_input":"2022-07-19T23:30:39.855131Z","iopub.status.idle":"2022-07-19T23:54:36.707721Z","shell.execute_reply.started":"2022-07-19T23:30:39.855093Z","shell.execute_reply":"2022-07-19T23:54:36.706661Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport pytorch_lightning\nfrom IPython.core.display import display\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger \nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.datasets import ImageFolder\n\n\n#from torchvision.datasets.utils import download_url\n#import tarfile\n\n# set the current directory as working directory\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n# setting of batch size\nBATCH_SIZE = 200 if torch.cuda.is_available() else 64\n\nclass LitCIFAR10(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS):\n        super().__init__()\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        \n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (3, 32, 32)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [   transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            ]\n        )\n        \n        # Define PyTorch model\n        self.model = nn.Sequential(\n            #CNN-v3\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:512 x 2 x 2\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:1024 x 1 x 1\n\n            nn.Flatten(), \n            nn.Dropout(p=0.2, inplace=False),\n            nn.Linear(1024, 10),\n        )\n        self.train_accuracy = Accuracy()\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y) # Calculate loss\n        preds = torch.argmax(logits, dim=1)\n        self.train_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n         # Set up cutom optimizer with weight decay\n        optimizer = torch.optim.Adam(self.model.parameters(),weight_decay=1e-4,lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=15)\n        return [optimizer], [lr_scheduler]\n\n    def validation_epoch_end(self, outputs):\n\n        return \n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        CIFAR10(self.data_dir, train=True, download=True)\n        CIFAR10(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            train_tfms =transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n                         transforms.RandomHorizontalFlip(), \n                         transforms.ToTensor(), \n                         transforms.Normalize(*stats,inplace=True)])\n            datasets = CIFAR10(self.data_dir, train=True, transform=train_tfms) # image augumention \n            self.train_data, self.val_data = random_split(datasets, [49000, 1000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.test_data = CIFAR10(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        train_dl = DataLoader(self.train_data, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n        return train_dl\n\n    def val_dataloader(self):\n        valid_dl = DataLoader(self.val_data, BATCH_SIZE, num_workers=2, pin_memory=True)\n        return valid_dl\n\n    def test_dataloader(self):\n        return DataLoader(self.test_data, batch_size=BATCH_SIZE)\n\n\nstart=datetime.now()\nmodel = LitCIFAR10()\n\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n#logger = TensorBoardLogger(\"lightning_logs\", name=None)\n\ntrainer = Trainer(\n    precision=16,\n    #fast_dev_run=False,\n    #deterministic=True,\n    gradient_clip_val=0.3,\n    #auto_lr_find=True,\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=50,\n    callbacks=[TQDMProgressBar(refresh_rate=20)],\n    #logger=logger,\n    logger=CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\nprint()\nprint(f\"The time cost in training model: {datetime.now()-start}\")\nprint()\ntrainer.test()\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\ndisplay(metrics.dropna(axis=1, how=\"all\"))\n#sn.relplot(data=metrics, kind=\"line\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-20T00:01:31.611326Z","iopub.execute_input":"2022-07-20T00:01:31.611777Z","iopub.status.idle":"2022-07-20T00:25:56.755012Z","shell.execute_reply.started":"2022-07-20T00:01:31.611740Z","shell.execute_reply":"2022-07-20T00:25:56.753905Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport pytorch_lightning\nfrom IPython.core.display import display\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger \nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.datasets import ImageFolder\n\n\n#from torchvision.datasets.utils import download_url\n#import tarfile\n\n# set the current directory as working directory\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n# setting of batch size\nBATCH_SIZE = 200 if torch.cuda.is_available() else 64\n\nclass LitCIFAR10(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS):\n        super().__init__()\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        \n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (3, 32, 32)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [   transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            ]\n        )\n        \n        # Define PyTorch model\n        self.model = nn.Sequential(\n            #CNN-v3\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:512 x 2 x 2\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:1024 x 1 x 1\n\n            nn.Flatten(), \n            nn.Dropout(p=0.2, inplace=False),\n            nn.Linear(1024, 10),\n        )\n        self.train_accuracy = Accuracy()\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y) # Calculate loss\n        preds = torch.argmax(logits, dim=1)\n        self.train_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n         # Set up cutom optimizer with weight decay\n        optimizer = torch.optim.Adam(self.model.parameters(),weight_decay=1e-4,lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=15)\n        return [optimizer], [lr_scheduler]\n\n    def validation_epoch_end(self, outputs):\n\n        return \n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        CIFAR10(self.data_dir, train=True, download=True)\n        CIFAR10(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            train_tfms =transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n                         transforms.RandomHorizontalFlip(), \n                         transforms.ToTensor(), \n                         transforms.Normalize(*stats,inplace=True)])\n            datasets = CIFAR10(self.data_dir, train=True, transform=train_tfms) # image augumention \n            self.train_data, self.val_data = random_split(datasets, [49000, 1000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.test_data = CIFAR10(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        train_dl = DataLoader(self.train_data, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n        return train_dl\n\n    def val_dataloader(self):\n        valid_dl = DataLoader(self.val_data, BATCH_SIZE, num_workers=2, pin_memory=True)\n        return valid_dl\n\n    def test_dataloader(self):\n        return DataLoader(self.test_data, batch_size=BATCH_SIZE)\n\n\nstart=datetime.now()\nmodel = LitCIFAR10()\n\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n#logger = TensorBoardLogger(\"lightning_logs\", name=None)\n\ntrainer = Trainer(\n    precision=16,\n    #fast_dev_run=False,\n    #deterministic=True,\n    gradient_clip_val=0.4,\n    #auto_lr_find=True,\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=50,\n    callbacks=[TQDMProgressBar(refresh_rate=20)],\n    #logger=logger,\n    logger=CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\nprint()\nprint(f\"The time cost in training model: {datetime.now()-start}\")\nprint()\ntrainer.test()\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\ndisplay(metrics.dropna(axis=1, how=\"all\"))\n#sn.relplot(data=metrics, kind=\"line\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport pytorch_lightning\nfrom IPython.core.display import display\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger \nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.datasets import ImageFolder\n\n\n#from torchvision.datasets.utils import download_url\n#import tarfile\n\n# set the current directory as working directory\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n# setting of batch size\nBATCH_SIZE = 200 if torch.cuda.is_available() else 64\n\nclass LitCIFAR10(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS):\n        super().__init__()\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        \n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (3, 32, 32)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [   transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            ]\n        )\n        \n        # Define PyTorch model\n        self.model = nn.Sequential(\n            #CNN-v3\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:512 x 2 x 2\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:1024 x 1 x 1\n\n            nn.Flatten(), \n            nn.Dropout(p=0.2, inplace=False),\n            nn.Linear(1024, 10),\n        )\n        self.train_accuracy = Accuracy()\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y) # Calculate loss\n        preds = torch.argmax(logits, dim=1)\n        self.train_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n         # Set up cutom optimizer with weight decay\n        optimizer = torch.optim.Adam(self.model.parameters(),weight_decay=1e-4,lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=15)\n        return [optimizer], [lr_scheduler]\n\n    def validation_epoch_end(self, outputs):\n\n        return \n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        CIFAR10(self.data_dir, train=True, download=True)\n        CIFAR10(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            train_tfms =transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n                         transforms.RandomHorizontalFlip(), \n                         transforms.ToTensor(), \n                         transforms.Normalize(*stats,inplace=True)])\n            datasets = CIFAR10(self.data_dir, train=True, transform=train_tfms) # image augumention \n            self.train_data, self.val_data = random_split(datasets, [49000, 1000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.test_data = CIFAR10(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        train_dl = DataLoader(self.train_data, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n        return train_dl\n\n    def val_dataloader(self):\n        valid_dl = DataLoader(self.val_data, BATCH_SIZE, num_workers=2, pin_memory=True)\n        return valid_dl\n\n    def test_dataloader(self):\n        return DataLoader(self.test_data, batch_size=BATCH_SIZE)\n\n\nstart=datetime.now()\nmodel = LitCIFAR10()\n\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n#logger = TensorBoardLogger(\"lightning_logs\", name=None)\n\ntrainer = Trainer(\n    precision=16,\n    #fast_dev_run=False,\n    #deterministic=True,\n    gradient_clip_val=0.4,\n    #auto_lr_find=True,\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=50,\n    callbacks=[TQDMProgressBar(refresh_rate=20)],\n    #logger=logger,\n    logger=CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\nprint()\nprint(f\"The time cost in training model: {datetime.now()-start}\")\nprint()\ntrainer.test()\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\ndisplay(metrics.dropna(axis=1, how=\"all\"))\n#sn.relplot(data=metrics, kind=\"line\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-20T00:26:16.136784Z","iopub.execute_input":"2022-07-20T00:26:16.137378Z","iopub.status.idle":"2022-07-20T00:50:35.052179Z","shell.execute_reply.started":"2022-07-20T00:26:16.137342Z","shell.execute_reply":"2022-07-20T00:50:35.051257Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport pytorch_lightning\nfrom IPython.core.display import display\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger \nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.datasets import ImageFolder\n\n\n#from torchvision.datasets.utils import download_url\n#import tarfile\n\n# set the current directory as working directory\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n# setting of batch size\nBATCH_SIZE = 200 if torch.cuda.is_available() else 64\n\nclass LitCIFAR10(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS):\n        super().__init__()\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        \n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (3, 32, 32)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [   transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            ]\n        )\n        \n        # Define PyTorch model\n        self.model = nn.Sequential(\n            #CNN-v3\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:512 x 2 x 2\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:1024 x 1 x 1\n\n            nn.Flatten(), \n            nn.Dropout(p=0.2, inplace=False),\n            nn.Linear(1024, 10),\n        )\n        self.train_accuracy = Accuracy()\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y) # Calculate loss\n        preds = torch.argmax(logits, dim=1)\n        self.train_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n         # Set up cutom optimizer with weight decay\n        optimizer = torch.optim.Adam(self.model.parameters(),weight_decay=1e-4,lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=15)\n        return [optimizer], [lr_scheduler]\n\n    def validation_epoch_end(self, outputs):\n\n        return \n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        CIFAR10(self.data_dir, train=True, download=True)\n        CIFAR10(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            train_tfms =transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n                         transforms.RandomHorizontalFlip(), \n                         transforms.ToTensor(), \n                         transforms.Normalize(*stats,inplace=True)])\n            datasets = CIFAR10(self.data_dir, train=True, transform=train_tfms) # image augumention \n            self.train_data, self.val_data = random_split(datasets, [49000, 1000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.test_data = CIFAR10(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        train_dl = DataLoader(self.train_data, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n        return train_dl\n\n    def val_dataloader(self):\n        valid_dl = DataLoader(self.val_data, BATCH_SIZE, num_workers=2, pin_memory=True)\n        return valid_dl\n\n    def test_dataloader(self):\n        return DataLoader(self.test_data, batch_size=BATCH_SIZE)\n\n\nstart=datetime.now()\nmodel = LitCIFAR10()\n\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n#logger = TensorBoardLogger(\"lightning_logs\", name=None)\n\ntrainer = Trainer(\n    precision=16,\n    #fast_dev_run=False,\n    #deterministic=True,\n    gradient_clip_val=0.5,\n    #auto_lr_find=True,\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=50,\n    callbacks=[TQDMProgressBar(refresh_rate=20)],\n    #logger=logger,\n    logger=CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\nprint()\nprint(f\"The time cost in training model: {datetime.now()-start}\")\nprint()\ntrainer.test()\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\ndisplay(metrics.dropna(axis=1, how=\"all\"))\n#sn.relplot(data=metrics, kind=\"line\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-20T01:21:00.699629Z","iopub.execute_input":"2022-07-20T01:21:00.700524Z","iopub.status.idle":"2022-07-20T01:45:01.833214Z","shell.execute_reply.started":"2022-07-20T01:21:00.700473Z","shell.execute_reply":"2022-07-20T01:45:01.831754Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport pytorch_lightning\nfrom IPython.core.display import display\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger \nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.datasets import ImageFolder\n\n\n#from torchvision.datasets.utils import download_url\n#import tarfile\n\n# set the current directory as working directory\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n# setting of batch size\nBATCH_SIZE = 200 if torch.cuda.is_available() else 64\n\nclass LitCIFAR10(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS):\n        super().__init__()\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        \n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (3, 32, 32)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [   transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            ]\n        )\n        \n        # Define PyTorch model\n        self.model = nn.Sequential(\n            #CNN-v3\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:512 x 2 x 2\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:1024 x 1 x 1\n\n            nn.Flatten(), \n            nn.Dropout(p=0.4, inplace=False),\n            nn.Linear(1024, 10),\n        )\n        self.train_accuracy = Accuracy()\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y) # Calculate loss\n        preds = torch.argmax(logits, dim=1)\n        self.train_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n         # Set up cutom optimizer with weight decay\n        optimizer = torch.optim.Adam(self.model.parameters(),weight_decay=1e-4,lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=15)\n        return [optimizer], [lr_scheduler]\n\n    def validation_epoch_end(self, outputs):\n\n        return \n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        CIFAR10(self.data_dir, train=True, download=True)\n        CIFAR10(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            train_tfms =transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n                         transforms.RandomHorizontalFlip(), \n                         transforms.ToTensor(), \n                         transforms.Normalize(*stats,inplace=True)])\n            datasets = CIFAR10(self.data_dir, train=True, transform=train_tfms) # image augumention \n            self.train_data, self.val_data = random_split(datasets, [49000, 1000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.test_data = CIFAR10(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        train_dl = DataLoader(self.train_data, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n        return train_dl\n\n    def val_dataloader(self):\n        valid_dl = DataLoader(self.val_data, BATCH_SIZE, num_workers=2, pin_memory=True)\n        return valid_dl\n\n    def test_dataloader(self):\n        return DataLoader(self.test_data, batch_size=BATCH_SIZE)\n\n\nstart=datetime.now()\nmodel = LitCIFAR10()\n\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n#logger = TensorBoardLogger(\"lightning_logs\", name=None)\n\ntrainer = Trainer(\n    precision=16,\n    #fast_dev_run=False,\n    #deterministic=True,\n    gradient_clip_val=0.5,\n    #auto_lr_find=True,\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=50,\n    callbacks=[TQDMProgressBar(refresh_rate=20)],\n    #logger=logger,\n    logger=CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\nprint()\nprint(f\"The time cost in training model: {datetime.now()-start}\")\nprint()\ntrainer.test()\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\ndisplay(metrics.dropna(axis=1, how=\"all\"))\n#sn.relplot(data=metrics, kind=\"line\")\n","metadata":{"execution":{"iopub.status.busy":"2022-07-20T01:47:15.232631Z","iopub.execute_input":"2022-07-20T01:47:15.233053Z","iopub.status.idle":"2022-07-20T02:11:11.898310Z","shell.execute_reply.started":"2022-07-20T01:47:15.233018Z","shell.execute_reply":"2022-07-20T02:11:11.897217Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport pytorch_lightning\nfrom IPython.core.display import display\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger \nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.datasets import ImageFolder\n\n\n#from torchvision.datasets.utils import download_url\n#import tarfile\n\n# set the current directory as working directory\nPATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n# setting of batch size\nBATCH_SIZE = 200 if torch.cuda.is_available() else 64\n\nclass LitCIFAR10(LightningModule):\n    def __init__(self, data_dir=PATH_DATASETS):\n        super().__init__()\n        # Set our init args as class attributes\n        self.data_dir = data_dir\n        \n        # Hardcode some dataset specific attributes\n        self.num_classes = 10\n        self.dims = (3, 32, 32)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [   transforms.ToTensor(),\n                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            ]\n        )\n        \n        # Define PyTorch model\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:512 x 2 x 2\n            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output:1024 x 1 x 1\n\n            nn.Flatten(), \n            nn.Dropout(p=0.2, inplace=False),\n            nn.Linear(1024, 10),\n        )\n        self.train_accuracy = Accuracy()\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y) # Calculate loss\n        preds = torch.argmax(logits, dim=1)\n        self.train_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", self.train_accuracy, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n         # Set up cutom optimizer with weight decay\n        optimizer = torch.optim.Adam(self.model.parameters(),weight_decay=1e-4,lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=15)\n        return [optimizer], [lr_scheduler]\n\n    def validation_epoch_end(self, outputs):\n\n        return \n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def prepare_data(self):\n        # download\n        CIFAR10(self.data_dir, train=True, download=True)\n        CIFAR10(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n            train_tfms =transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n                         transforms.RandomHorizontalFlip(), \n                         transforms.ToTensor(), \n                         transforms.Normalize(*stats,inplace=True)])\n            datasets = CIFAR10(self.data_dir, train=True, transform=train_tfms) # image augumention \n            self.train_data, self.val_data = random_split(datasets, [49000, 1000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.test_data = CIFAR10(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        train_dl = DataLoader(self.train_data, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n        return train_dl\n\n    def val_dataloader(self):\n        valid_dl = DataLoader(self.val_data, BATCH_SIZE, num_workers=2, pin_memory=True)\n        return valid_dl\n\n    def test_dataloader(self):\n        return DataLoader(self.test_data, batch_size=BATCH_SIZE)\n\n\nstart=datetime.now()\nmodel = LitCIFAR10()\n\n\n#logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n#logger = TensorBoardLogger(\"lightning_logs\", name=None)\n\ntrainer = Trainer(\n    precision=16,\n    #fast_dev_run=False,\n    #deterministic=True,\n    gradient_clip_val=0.5,\n    #auto_lr_find=True,\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=50,\n    callbacks=[TQDMProgressBar(refresh_rate=20)],\n    #logger=logger,\n    logger=CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\nprint()\nprint(f\"The time cost in training model: {datetime.now()-start}\")\nprint()\ntrainer.test()\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\ndisplay(metrics.dropna(axis=1, how=\"all\"))\n#sn.relplot(data=metrics, kind=\"line\")\n","metadata":{},"execution_count":null,"outputs":[]}]}